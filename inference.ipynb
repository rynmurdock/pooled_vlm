{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJjAmplaD4FG"
   },
   "outputs": [],
   "source": [
    "!pip install datasets timm 'transformers==4.37.2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "At_60t8A92zP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "dda6bb6b93f446d59d72ff96ff7a853e",
      "b44eba0bbe4c43a1b0f721e678b4d283",
      "e437079090dc473982e44067232d5a8a",
      "c8e975345e394d50835853f171d168b5",
      "dce79a5e12054f3b81f81f75ae16e496",
      "0a1c58f845d8401f803dc82650ec5067",
      "be5c59d8ae394c148dcd0c6cf9103c6c",
      "851538e4adf34b9a923d109af48bf10f",
      "82f1095705fc42f1a0bd5c419efd5fb5",
      "1e8b80093b474466a159d263385af6c6",
      "bcd3745ef57746678408765d98eb4e05"
     ]
    },
    "id": "Fjz_CIWCgHIN",
    "outputId": "c64d772a-5df0-4f7e-e535-387777099423"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "WARNING:transformers_modules.OpenGVLab.InternVL2-4B.7f49802f5bf1e6e3d20b6f69268701c7eb67e037.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.OpenGVLab.InternVL2-4B.7f49802f5bf1e6e3d20b6f69268701c7eb67e037.modeling_phi3:Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda6bb6b93f446d59d72ff96ff7a853e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# If you want to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\n",
    "path = 'OpenGVLab/InternVL2-4B'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3ep4uzx80DJ",
    "outputId": "012b5503-8950-4daf-b07f-cbf5c5da2e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Please describe the image shortly.\n",
      "Assistant: The image portrays a classical painting exhibiting a figure with long, flowing red hair against a backdrop of a dark, starry night sky. The style leans heavily into surrealism, with the figure being enveloped in shadows and a blurred haze, creating an impressionistic effect. The blurry quality of the painting suggests movement and depth, evoking a sense of mystery and intrigue. The use of dark and muted tones adds a dramatic and somber atmosphere to the artwork.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('/content/imageData (10).png', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# single-image single-round conversation (单图单轮对话)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncu5qq0wEfKN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgjhC95CDb26",
    "outputId": "709bb3ad-fe29-41a3-dd4d-d3bab9fb21ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '000000537656', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=375x500 at 0x798C28DEAA70>, 'conversations': [{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'In the image, a man is captured in a moment of quiet contemplation. He is standing in a room that exudes a sense of comfort and familiarity. The room is adorned with posters on the wall, adding a personal touch to the space.\\n\\nThe man is dressed in a gray vest, which contrasts with his black and white striped shirt. The vest is buttoned up, suggesting a formal or semi-formal occasion. Adding to his attire is a black bowtie, neatly tied and hanging down his chest.\\n\\nHis face is framed by a mustache, adding a touch of sophistication to his look. He is wearing glasses, which rest comfortably on his nose. His gaze is directed off to the side, indicating that he is lost in thought or perhaps observing something out of frame.\\n\\nThe overall image paints a picture of a thoughtful individual, dressed for a special occasion, standing in a room that feels like home.'}], 'data_source': 'llava_recap_118k'}]\n",
      "[{'id': '000000000009', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x798C28DE9990>, 'conversations': [{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'The image shows a meal served in a blue tray with compartments. In the top left compartment, there is a slice of bread with a spread that appears to be butter, accompanied by a few almonds and a slice of what looks like a baked potato or sweet potato. The top right compartment contains a variety of fruits, including what seems to be pineapple, orange slices, and possibly a piece of melon.\\n\\nIn the bottom left compartment, there is a piece of bread with a spread that could be butter or margarine, and a small portion of what might be a meatball or a similar type of meat covered in a sauce. The bottom right compartment contains a serving of broccoli, which appears to be steamed or lightly cooked.\\n\\nThe meal is presented in a way that suggests it is a balanced meal, with a variety of food groups represented carbohydrates (bread), protein (meatball), healthy fats (almonds and butter), and fruits and vegetables (broccoli and the fruit assortment). The image is a close-up photograph with a focus on the food, and the colors are vibrant, indicating freshness.'}], 'data_source': 'llava_recap_118k'}]\n",
      "[{'id': '000000537667', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x798D08F18E50>, 'conversations': [{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': \"The image is a black and white photograph capturing a dynamic moment from a baseball game. The central focus is on three players the runner, the baseman, and the umpire. The runner, dressed in a uniform with the number 24, is in the midst of sliding into the base, his body low to the ground and his arms extended forward. The baseman, wearing a uniform with the number 2, is in a crouched position, his gloved hand extended towards the runner in an attempt to tag him out. The umpire, standing behind the baseman, is attentively observing the play, ready to make a call.\\n\\nThe photograph is encased in a gold frame, adding a touch of elegance to the presentation. The background of the image reveals a well-maintained baseball field, complete with a dirt infield and a grass outfield. The perspective of the photo suggests it was taken from the third base side of the field, providing a clear view of the action.\\n\\nThe image is a testament to the intensity and excitement of baseball, capturing a moment of suspense as the outcome of the play hangs in the balance. It's a snapshot of a game that's as much about strategy and skill as it is about athleticism and teamwork.\"}], 'data_source': 'llava_recap_118k'}]\n",
      "[{'id': '000000000025', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x426 at 0x798D08F18AF0>, 'conversations': [{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': \"In the heart of a verdant enclosure, a majestic giraffe stands tall, its long neck reaching upwards towards the sky. The giraffe, adorned with a coat of brown and white spots, is positioned on the right side of the image. It faces towards the left, its attention seemingly captured by something beyond our view.\\n\\nThe enclosure itself is a lush expanse of green, dotted with trees that provide a natural backdrop to this scene. The giraffe's long legs are firmly planted on the ground, and its tail sways gently in the breeze.\\n\\nIn the background, on the left side of the image, another giraffe can be seen. This one is slightly smaller and is facing towards the right side of the image. Its presence adds a sense of depth and scale to the scene.\\n\\nThe image captures a moment of tranquility in the life of these magnificent creatures, offering a glimpse into their world.\"}], 'data_source': 'llava_recap_118k'}]\n",
      "[{'id': '000000000034', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x425 at 0x798D08F19DE0>, 'conversations': [{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': \"The image shows a zebra grazing in a grassy field. The zebra is facing to the right, with its head lowered to the ground, indicating that it is eating. The zebra's coat is a pattern of black and white stripes, which is characteristic of the species. The mane on the zebra's neck is standing upright, and the tail is visible, hanging down. The background is a mix of green grass and patches of bare earth, suggesting a natural, outdoor setting. The lighting in the image is bright, indicating that it is likely daytime. There are no other animals or objects in the immediate vicinity of the zebra, and the focus is solely on the zebra itself. The image is a photograph, capturing a moment in the life of the zebra in its natural habitat.\"}], 'data_source': 'llava_recap_118k'}]\n",
      "[[tensor([[[0.0078, 0.0078, 0.0078,  ..., 0.5333, 0.5216, 0.5176],\n",
      "         [0.0078, 0.0078, 0.0078,  ..., 0.5294, 0.5255, 0.5216],\n",
      "         [0.0078, 0.0078, 0.0039,  ..., 0.5333, 0.5294, 0.5216],\n",
      "         ...,\n",
      "         [0.0275, 0.0275, 0.0314,  ..., 0.0078, 0.0000, 0.0118],\n",
      "         [0.0235, 0.0235, 0.0275,  ..., 0.0235, 0.0157, 0.0118],\n",
      "         [0.0157, 0.0196, 0.0235,  ..., 0.0392, 0.0392, 0.0510]],\n",
      "\n",
      "        [[0.0824, 0.0824, 0.0824,  ..., 0.6706, 0.6667, 0.6627],\n",
      "         [0.0824, 0.0824, 0.0824,  ..., 0.6706, 0.6667, 0.6667],\n",
      "         [0.0824, 0.0824, 0.0863,  ..., 0.6745, 0.6706, 0.6667],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0078,  ..., 0.0353, 0.0157, 0.0353],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4510, 0.4510, 0.4510,  ..., 0.7490, 0.7373, 0.7333],\n",
      "         [0.4510, 0.4510, 0.4510,  ..., 0.7569, 0.7529, 0.7412],\n",
      "         [0.4510, 0.4510, 0.4510,  ..., 0.7765, 0.7725, 0.7686],\n",
      "         ...,\n",
      "         [0.0275, 0.0275, 0.0235,  ..., 0.1373, 0.1176, 0.1373],\n",
      "         [0.0235, 0.0235, 0.0275,  ..., 0.1255, 0.1137, 0.0980],\n",
      "         [0.0157, 0.0196, 0.0235,  ..., 0.1255, 0.1137, 0.1176]]])], ['The image shows a meal served in a blue tray with compartments. In the top left compartment, there is a slice of bread with a spread that appears to be butter, accompanied by a few almonds and a slice of what looks like a baked potato or sweet potato. The top right compartment contains a variety of fruits, including what seems to be pineapple, orange slices, and possibly a piece of melon.\\n\\nIn the bottom left compartment, there is a piece of bread with a spread that could be butter or margarine, and a small portion of what might be a meatball or a similar type of meat covered in a sauce. The bottom right compartment contains a serving of broccoli, which appears to be steamed or lightly cooked.\\n\\nThe meal is presented in a way that suggests it is a balanced meal, with a variety of food groups represented carbohydrates (bread), protein (meatball), healthy fats (almonds and butter), and fruits and vegetables (broccoli and the fruit assortment). The image is a close-up photograph with a focus on the food, and the colors are vibrant, indicating freshness.']]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a1c58f845d8401f803dc82650ec5067": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e8b80093b474466a159d263385af6c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82f1095705fc42f1a0bd5c419efd5fb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "851538e4adf34b9a923d109af48bf10f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b44eba0bbe4c43a1b0f721e678b4d283": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a1c58f845d8401f803dc82650ec5067",
      "placeholder": "​",
      "style": "IPY_MODEL_be5c59d8ae394c148dcd0c6cf9103c6c",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "bcd3745ef57746678408765d98eb4e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be5c59d8ae394c148dcd0c6cf9103c6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8e975345e394d50835853f171d168b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e8b80093b474466a159d263385af6c6",
      "placeholder": "​",
      "style": "IPY_MODEL_bcd3745ef57746678408765d98eb4e05",
      "value": " 2/2 [00:00&lt;00:00,  4.98it/s]"
     }
    },
    "dce79a5e12054f3b81f81f75ae16e496": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dda6bb6b93f446d59d72ff96ff7a853e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b44eba0bbe4c43a1b0f721e678b4d283",
       "IPY_MODEL_e437079090dc473982e44067232d5a8a",
       "IPY_MODEL_c8e975345e394d50835853f171d168b5"
      ],
      "layout": "IPY_MODEL_dce79a5e12054f3b81f81f75ae16e496"
     }
    },
    "e437079090dc473982e44067232d5a8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_851538e4adf34b9a923d109af48bf10f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_82f1095705fc42f1a0bd5c419efd5fb5",
      "value": 2
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
